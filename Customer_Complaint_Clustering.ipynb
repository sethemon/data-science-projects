{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROBLEM DESCRIPTION:\n",
    "---------------------------------------------------------------------\n",
    "The banks that are receiving customer complaints filed against them will analyse the complaint data to provide results on where the most complaints are being filed, what products/ services are producing the most complaints and other useful data. These datasets fall under the complaints of Credit reporting, Mortgage, Debt Collection, Consumer Loan and Banking Accounting.\n",
    "\n",
    "#### OBJECTIVE:\n",
    "---------------------------------------------------------------------\n",
    "Build a clustering model using Python language or other suitable tools to find how many similar complaints are there in relation to the same bank or service or product .This model will be used to assist banks in identifying the location and types of errors for resolution leading to increased customer satisfaction to drive revenue and profitability. \n",
    "\n",
    "#### DATASET SOURCE:\n",
    "https://www.kaggle.com/sebastienverpile/consumercomplaintsdata\n",
    "\n",
    "#### EXPECTED ACTIVITIES & OUTCOMES\n",
    "---------------------------------------------------------------------\n",
    "Your activities should include - performing various activities pertaining to the data such as, preparing the dataset for analysis; investigating the relationships in the data set using statistical techniques, visualization; creating a model; evaluating the performance of the clustering model.\n",
    "\n",
    "Demonstrate the Data Mining process with following activities:\n",
    "1. Problem statement\n",
    "2. Perform exploratory data analysis using the statistical techniques and box plot as applicable\n",
    "3. Preprocess the Data as the data from internet source may be noisy\n",
    "4. Evaluate the model performance using cohesion and separation\n",
    "5. Suggest ways of improving the model\n",
    "6. State all your assumptions clearly and provide clear explanations to explain your stand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set()\n",
    "\n",
    "# Plotly libraries\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import os\n",
    "import cufflinks as cf\n",
    "from plotly.offline import iplot, init_notebook_mode, plot\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_data = pd.read_csv('Consumer_Complaints.csv')\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.columns = customer_data.columns.str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data[['Issue','Sub-Issue','Product','Sub-Product','Company','State',\n",
    "               'Company Public Response','Consumer Consent Provided?','Date Received','Consumer Complaint Narrative',\n",
    "               'Company Response To Consumer','Submitted Via']].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 15 issues and sub-issues and companies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "customer_data['Issue'].str.strip(\"'\").value_counts()[0:15].iplot(kind='bar',title='Top 15 issues',fontsize=14,color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_data['Sub-Issue'].str.strip(\"'\").value_counts()[0:15].iplot(\n",
    "    kind ='bar',title='Top 15 Sub Issues',fontsize=14,color='#9370DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_data['Company'].str.strip(\"'\").value_counts()[0:15].iplot(\n",
    "    kind='bar',title='Top 15 Company',fontsize=14,color='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the most common response received from companies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = customer_data.groupby(['Company Response To Consumer']).size()\n",
    "pie_chart = go.Pie(labels=grouped.index,values=grouped,\n",
    "                  title='Company Response to the Customer')\n",
    "iplot([pie_chart])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which state received the largest number of complaints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "states = customer_data['State'].value_counts()\n",
    "\n",
    "scl = [\n",
    "    [0.0, 'rgb(242,240,247)'],\n",
    "    [0.2, 'rgb(218,218,235)'],\n",
    "    [0.4, 'rgb(188,189,220)'],\n",
    "    [0.6, 'rgb(158,154,200)'],\n",
    "    [0.8, 'rgb(117,107,177)'],\n",
    "    [1.0, 'rgb(84,39,143)']\n",
    "]\n",
    "\n",
    "data = [go.Choropleth(\n",
    "    colorscale = scl,\n",
    "    autocolorscale = False,\n",
    "    locations = states.index,\n",
    "    z = states.values,\n",
    "    locationmode = 'USA-states',\n",
    "    text = states.index,\n",
    "    marker = go.choropleth.Marker(\n",
    "        line = go.choropleth.marker.Line(\n",
    "            color = 'rgb(244,244,244)',\n",
    "            width = 2\n",
    "        )),\n",
    "    colorbar = go.choropleth.ColorBar(\n",
    "        title = \"Complaints\")\n",
    ")]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = go.layout.Title(\n",
    "        text = 'Complaints by State'\n",
    "    ),\n",
    "    geo = go.layout.Geo(\n",
    "        scope = 'usa',\n",
    "        projection = go.layout.geo.Projection(type = 'albers usa'),\n",
    "        showlakes = True,\n",
    "        lakecolor = 'rgb(100,149,237)'),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted features (checking % of missing values in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting the sum of null values and ordering.\n",
    "total = customer_data.isnull().sum().sort_values(ascending = False)  \n",
    "\n",
    "#getting the percent and order of null.\n",
    "percent = (customer_data.isnull().sum()/customer_data.isnull().count()*100).sort_values(ascending =False)\n",
    "\n",
    "# Concatenating the total and percent\n",
    "df = pd.concat([total , percent],axis =1,keys=['Total' ,'Percent'])\n",
    "\n",
    "# Returning values of nulls different of 0\n",
    "(df[~(df['Total'] == 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on above % rate , implementing the below cleanup activity - \n",
    "1. Remove Records from ZIP code since Location information can be retrieved from State features\n",
    "3. Remove Sub-issue, Consumer complaint narrative, Company public response, Tags, Consumer consent provided? as the missing % is high and their relation with Product is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing missing values and eradicating missing value features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complain = customer_data.drop(['Sub-Issue','Consumer Complaint Narrative','Date Received','Date Sent To Company',\n",
    "                                 'Company Public Response','Zip Code','Tags','Consumer Consent Provided?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_complain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_complain[['Product','Sub-Product','Issue','Company','State',\n",
    "               'Company Response To Consumer','Submitted Via','Timely Response?','Consumer Disputed?']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_complain.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we select below features which has complete data to support our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Label-Encoder to convert data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = defaultdict(LabelEncoder)\n",
    "labeled_df = train.apply(lambda x: encoder_dict[x.name].fit_transform(x))\n",
    "# train_enc = train.apply(labelencoder.fit_transform)\n",
    "labeled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labeled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labeled_df.drop(['Product'], 1)  #independent columns\n",
    "y = labeled_df['Product']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SelectKBest to analyze top best features related to Product using Chi square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  # naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  # print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "fit = bestfeatures.fit(X_scaled,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  # naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  # print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top best features selected for clustering using the Feature selection method above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc = labeled_df.drop(['Submitted Via','Consumer Disputed?','Company Response To Consumer','Timely Response?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_enc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: using the PCA components of the dataset for K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling using Standard scalar for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "train_std = scalar.fit_transform(train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,7), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "plt.title('Explained variance by components')\n",
    "plt.xlabel('No of components')\n",
    "plt.ylabel('Cumulative Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The graph shows the amount of variance captured (on the y-axis) depending on the number of components we include (the x-axis). A rule of thumb is to preserve around 80 % of the variance. So, in this instance, we decide to keep 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pca = pca.transform(train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN K-MEANS on PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for num_clusters in list(range(2,11)):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=\"k-means++\", verbose=0)\n",
    "    cluster_labels = kmeans.fit_predict(scores_pca)\n",
    "    cost.append(kmeans.inertia_)\n",
    "    print(f\"=============================== Cluster of {num_clusters} ====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_bar = np.array([i for i in range(2,11,1)])\n",
    "plt.plot(y_bar, cost)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"K-Means cost\")\n",
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since k->3 from graph, run K-Means with n_clusters=3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca = KMeans(n_clusters=3, init=\"k-means++\", verbose=0)\n",
    "kmeans_pca.fit_predict(scores_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = labeled_df.apply(lambda x: encoder_dict[x.name].inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_pca_kmeans = pd.concat([customer_df.reset_index(drop=True), pd.DataFrame(scores_pca)], axis=1)\n",
    "customer_df_pca_kmeans.columns.values[-5: ] = ['Component1','Component2','Component3','Component4','Component5']\n",
    "customer_df_pca_kmeans['PCA_KMeans_Clusters'] = kmeans_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_pca_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_pca_kmeans['Cluster'] = customer_df_pca_kmeans['PCA_KMeans_Clusters'].map({0:'first',1:'second',2:'third'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the segments with respect to the first two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = customer_df_pca_kmeans['Component1']\n",
    "y_axis = customer_df_pca_kmeans['Component2']\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x_axis, y_axis, hue=customer_df_pca_kmeans['Cluster'], palette=['g','r','y'])\n",
    "plt.title('Clusters by PCA components [1 and 2]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_axis = customer_df_pca_kmeans['Component2']\n",
    "y_axis = customer_df_pca_kmeans['Component3']\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x_axis, y_axis, hue=customer_df_pca_kmeans['Cluster'], palette=['g','r','y'])\n",
    "plt.title('Clusters by PCA components [2 and 3]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = customer_df_pca_kmeans['Component3']\n",
    "y_axis = customer_df_pca_kmeans['Component4']\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x_axis, y_axis, hue=customer_df_pca_kmeans['Cluster'], palette=['g','r','y'])\n",
    "plt.title('Clusters by PCA components [3 and 4]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = customer_df_pca_kmeans['Component4']\n",
    "y_axis = customer_df_pca_kmeans['Component5']\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x_axis, y_axis, hue=customer_df_pca_kmeans['Cluster'], palette=['g','r','y'])\n",
    "plt.title('Clusters by PCA components [4 and 5]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_axis = customer_df_pca_kmeans['Component1']\n",
    "y_axis = customer_df_pca_kmeans['Component5']\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x_axis, y_axis, hue=customer_df_pca_kmeans['Cluster'], palette=['g','r','y'])\n",
    "plt.title('Clusters by PCA components [1 and 5]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count per cluster\n",
    "cluster_pca_df = pd.DataFrame(customer_df_pca_kmeans['Cluster'].value_counts())\n",
    "sns.barplot(x=cluster_pca_df.index, y=customer_df_pca_kmeans['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Products segregated per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=customer_df_pca_kmeans['Product'],order=customer_df_pca_kmeans['Product'].value_counts().index,hue=customer_df_pca_kmeans['Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Issues segregated per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=customer_df_pca_kmeans['Issue'],order=customer_df_pca_kmeans['Issue'].value_counts()[:5].index,hue=customer_df_pca_kmeans['Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Sub-Products segregated per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=customer_df_pca_kmeans['Sub-Product'],order=customer_df_pca_kmeans['Sub-Product'].value_counts()[:5].index,hue=customer_df_pca_kmeans['Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Companies segregated per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=customer_df_pca_kmeans['Company'],order=customer_df_pca_kmeans['Company'].value_counts()[:5].index,hue=customer_df_pca_kmeans['Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the Above charts we can clearly see Companies have some relationship with same type of compaints clusters like \n",
    "1. Bank Of America - maximum complaints in cluster 1 and rest in cluster 0 and 2 which are internally related to the complaints like Loan Service and Loan Modification.\n",
    "2. Wells Fargo & Company - have max complaints in cluster 1 and rest in 0 and 2 which are internally related to the complaints like Debts and Account opening.\n",
    "3. JPMorgan Chase - have max complaints in cluster 1 and few in cluster 1, cluster 2 which are internally related to the complaints like Account Opening and Cont'd attempts collect debt and Loans.\n",
    "4. Ocwen - have max complaints in cluster 0 and 1 which are internally related to the complaints like Loan Service and Loan Modification and related to Mortgage products.\n",
    "5. Ciibank - have evenly distributed complaints in cluster 2, cluster 1 and cluster 0 which are internally related to the complaints like Loan Service and Loan Modification and Disclosure verification of debt\n",
    "\n",
    "\n",
    "##### Top products - Mortgage and Debt Collection are distributed amongst cluster 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Approach: using K-Modes clustering technique for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 features selected for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_enc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing K by comparing Cost against each K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "scaled_train = scalar.fit_transform(train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "\n",
    "for num_clusters in list(range(2,6)):\n",
    "    kmode = KModes(n_clusters=num_clusters, init=\"Huang\", n_init=1, verbose=0)\n",
    "    kmode.fit_predict(train_enc)\n",
    "    cost.append(kmode.cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_bar = np.array([i for i in range(2,6,1)])\n",
    "plt.plot(y_bar, cost)\n",
    "# RANDOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly we can see that the Elbow method show that the K value is 3 in the graph, so we select K=3 for our KModes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient measures how similar an object is to it's own cluster(cohesion) compared to other clusters (separation) It's value ranges from -1 to 1. A value close to 1 indicates that the object is well matched to its cluster and poorly matched to neighbouring clusters. If most of the objects have high Sil Coefficient, clusters are formed correctly. If more negative valued points are present, there is some issue in the clustering configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_coefficients = []\n",
    "\n",
    "for num_clusters in range(2, 5):\n",
    "    kmodes_score = KModes(n_clusters=num_clusters, init = \"Huang\", n_init = 1, verbose=1)\n",
    "    kmodes_score.fit(scaled_train)\n",
    "    silhouette_avg = silhouette_score(scaled_train, kmodes_score.labels_)\n",
    "    print(f\"For n_clusters = {num_clusters}, The average silhouette_score is : {silhouette_avg}\")\n",
    "    silhouette_coefficients.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 5), silhouette_coefficients)\n",
    "plt.xticks(range(2, 5))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For n_clusters = 3, The average silhouette_score is : 0.12128887593549928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Mode with \"Huang\" initialization for K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_rand = KModes(n_clusters=3, init=\"Huang\", n_init=5, verbose=0)\n",
    "fitClusters_rand = km_rand.fit_predict(train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoded = labeled_df.apply(lambda x: encoder_dict[x.name].inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoded = train_decoded.reset_index()\n",
    "clustersDf = pd.DataFrame(fitClusters_rand)\n",
    "clustersDf.columns = ['cluster_predicted']\n",
    "combinedDf = pd.concat([train_decoded, clustersDf], axis = 1).reset_index()\n",
    "combinedDf = combinedDf.drop(['index', 'level_0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count per cluster\n",
    "cluster_df = pd.DataFrame(combinedDf['cluster_predicted'].value_counts())\n",
    "sns.barplot(x=cluster_df.index, y=cluster_df['cluster_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = combinedDf[combinedDf['cluster_predicted'] == 0]\n",
    "cluster_1 = combinedDf[combinedDf['cluster_predicted'] == 1]\n",
    "cluster_2 = combinedDf[combinedDf['cluster_predicted'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=combinedDf['Product'],order=combinedDf['Product'].value_counts().index,hue=combinedDf['cluster_predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,7))\n",
    "sns.countplot(x=combinedDf['Sub-Product'],order=combinedDf['Sub-Product'].value_counts()[:5].index,hue=combinedDf['cluster_predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=combinedDf['Issue'],order=combinedDf['Issue'].value_counts()[:5].index,hue=combinedDf['cluster_predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,5))\n",
    "sns.countplot(x=combinedDf['Company'],order=combinedDf['Company'].value_counts()[:5].index,hue=combinedDf['cluster_predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the Above charts we can clearly see Companies have some relationship with same type of compaints clusters like \n",
    "1. Bank Of America - maximum complaints in cluster 1 and few cluster 2 which are internally related to the complaints like Loan Service and Loan Modification \n",
    "2. Wells Fargo & Company - have max complaints in cluster 1 and 2 which are internally related to the complaints like Loan Modification and Account opening\n",
    "3. JPMorgan Chase - have max complaints in cluster 1, cluster 2 and few in cluster 0 which are internally related to the complaints like Account Opening and Cont'd attempts collect debt and Loans\n",
    "4. Ocwen - have max complaints in cluster 1 which are internally related to the complaints like Loan Service and Loan Modification \n",
    "5. Ciibank - have averaged complaints in cluster 0 and cluster 2 and most in cluster 1 which are internally related to the complaints like Loan Service and Loan Modification and Disclosure verification of debt\n",
    "\n",
    "\n",
    "##### Most of them are related to Mortgage and Debt Collection products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combinedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of Improvements\n",
    "1. We can use K- Medoid instead of K-Means to reduce the effect of outliers.\n",
    "2. We can use Feature Weightage using domain knowledge to improve the clustering. We have used this.\n",
    "3. There are so many missing data in the input. We have eliminated 2 products with more missing values. The model can be improved by getting better data.\n",
    "4. Principal Component Analysis can be used to identify the most important features of the dataset. We have used this as well in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
